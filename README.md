
# nanoGPT_Chinese
nanoGPT：

    简介：从0到1实现自己的大语言模型（GPT-2）
    仓库地址：https://github.com/karpathy/nanoGPT
    作者：Andrej Karpathy（https://karpathy.ai/），曾任特斯拉人工智能和自动驾驶部门（Autopilot）负责人，2023年加入 OpenAI，2024年离职。

    | 模型         | 参数数量 |
    | ------------| -------- |
    | gpt2        | 124M     |
    | gpt2-medium | 350M     |
    | gpt2-large  | 774M     |
    | gpt2-xl     | 1558M    |

nanoGPT_Chinese基于nanoGPT，收集了一个中文医疗数据集，并进行了中文语料的训练。
## 源数据集
	[cMedQA2](https://github.com/hejunqing/webMedQA)  包含约10万个医学相关问题，及对应的约20万个回答。
    webMedQA 一个医学在线问答数据集，包含6万个问题和31万个回答。

## 使用


## todos
